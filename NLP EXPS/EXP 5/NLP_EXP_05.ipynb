{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "NAME : PAREENITA A.SHIRSATH    PRN  :  221101062  ROLL.NO : 57  \n",
        "B.E.A.I.&.D.S.\n",
        "\n",
        "NLP EXPERIMENT NO : 05"
      ],
      "metadata": {
        "id": "jVcu-mKLABJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "def tokenize_text(text):\n",
        "    \"\"\"\n",
        "    Tokenize the input text into a list of words.\n",
        "    Lowercases and removes punctuation.\n",
        "    \"\"\"\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text.lower().split()\n",
        "\n",
        "def build_ngram_model(sentences, n):\n",
        "    \"\"\"\n",
        "    Build an N-Gram model from the input sentences.\n",
        "\n",
        "    Args:\n",
        "        sentences (list): List of sentences (strings).\n",
        "        n (int): N-gram size (1=unigram, 2=bigram, 3=trigram, etc.).\n",
        "\n",
        "    Returns:\n",
        "        dict: Nested defaultdict representing the N-gram counts.\n",
        "    \"\"\"\n",
        "    model = defaultdict(lambda: defaultdict(int))\n",
        "    for sentence in sentences:\n",
        "        tokens = tokenize_text(sentence)\n",
        "        if n > 1:\n",
        "            tokens = [\"<START>\"] * (n - 1) + tokens\n",
        "        tokens = tokens + [\"<END>\"]\n",
        "        for i in range(len(tokens) - n + 1):\n",
        "            context = tuple(tokens[i:i + n - 1]) if n > 1 else ()\n",
        "            next_word = tokens[i + n - 1]\n",
        "            model[context][next_word] += 1\n",
        "    return model\n",
        "\n",
        "def predict_next_word(model, context):\n",
        "    if context not in model or not model[context]:\n",
        "        return \"unknown\"\n",
        "    return max(model[context], key=model[context].get)\n",
        "\n",
        "def bigram_probability(model, context, word):\n",
        "    total_count = sum(model[context].values())\n",
        "    word_count = model[context][word]\n",
        "    return word_count / total_count if total_count > 0 else 0\n",
        "\n",
        "# Sample sentences\n",
        "sentences = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Artificial intelligence is transforming the world rapidly.\",\n",
        "    \"Data science involves statistics, programming, and domain knowledge.\",\n",
        "    \"Python is a popular programming language for developers.\",\n",
        "    \"Machine learning models can predict outcomes based on data.\"\n",
        "]\n",
        "\n",
        "# Build bigram model\n",
        "bigram_model = build_ngram_model(sentences, 2)\n",
        "\n",
        "contexts_bigram = [\n",
        "    (\"<START>\",),\n",
        "    (\"the\",),\n",
        "    (\"artificial\",),\n",
        "    (\"data\",),\n",
        "    (\"python\",),\n",
        "    (\"machine\",),\n",
        "    (\"dog\",)\n",
        "]\n",
        "\n",
        "print(f\"{'Context':15} | {'Predicted Next Word':20} | {'Bigram Count':12} | {'Bigram Probability':18} | {'Most Probable Next Word':22}\")\n",
        "print(\"-\"*100)\n",
        "\n",
        "overall_counts = defaultdict(int)\n",
        "\n",
        "for context in contexts_bigram:\n",
        "    if context in bigram_model and bigram_model[context]:\n",
        "        predicted_word = predict_next_word(bigram_model, context)\n",
        "        count_bigram = bigram_model[context][predicted_word]\n",
        "        prob_bigram = bigram_probability(bigram_model, context, predicted_word)\n",
        "        most_probable_word = max(bigram_model[context], key=lambda w: bigram_probability(bigram_model, context, w))\n",
        "\n",
        "        # Collect counts for overall most probable word later\n",
        "        overall_counts[predicted_word] += count_bigram\n",
        "\n",
        "        context_str = \" \".join(context)\n",
        "        print(f\"{context_str:15} | {predicted_word:20} | {count_bigram:<12} | {prob_bigram:<18.4f} | {most_probable_word:22}\")\n",
        "    else:\n",
        "        context_str = \" \".join(context)\n",
        "        print(f\"{context_str:15} | {'No data':20} | {'-':12} | {'-':18} | {'-':22}\")\n",
        "\n",
        "print(\"-\"*100)\n",
        "\n",
        "# Find overall most probable next word across all contexts based on counts\n",
        "if overall_counts:\n",
        "    overall_most_probable_word = max(overall_counts, key=overall_counts.get)\n",
        "    print(f\"Overall most probable next word across contexts: {overall_most_probable_word}\")\n",
        "else:\n",
        "    print(\"No overall most probable next word found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vl6-JTzRAXrq",
        "outputId": "738b671d-9d50-4845-8e9c-10dc3d2de3ea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context         | Predicted Next Word  | Bigram Count | Bigram Probability | Most Probable Next Word\n",
            "----------------------------------------------------------------------------------------------------\n",
            "<START>         | the                  | 1            | 0.2000             | the                   \n",
            "the             | quick                | 1            | 0.3333             | quick                 \n",
            "artificial      | intelligence         | 1            | 1.0000             | intelligence          \n",
            "data            | science              | 1            | 0.5000             | science               \n",
            "python          | is                   | 1            | 1.0000             | is                    \n",
            "machine         | learning             | 1            | 1.0000             | learning              \n",
            "dog             | <END>                | 1            | 1.0000             | <END>                 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "Overall most probable next word across contexts: the\n"
          ]
        }
      ]
    }
  ]
}